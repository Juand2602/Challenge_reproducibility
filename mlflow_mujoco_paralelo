import gym
import numpy as np
import imageio
import matplotlib.pyplot as plt
import os
import multiprocessing as mp
import mlflow
import mlflow.sklearn
from mlflow.tracking import MlflowClient

# Pol√≠tica MLP simple
class Policy:
    def __init__(self, input_size, hidden_size, output_size):
        self.shapes = [(input_size, hidden_size), (hidden_size,),
                       (hidden_size, hidden_size), (hidden_size,),
                       (hidden_size, output_size), (output_size,)]
        self.params = self._init_params()

    def _init_params(self):
        return [np.random.randn(*shape) * 0.1 for shape in self.shapes]

    def set_params(self, params):
        self.params = params

    def get_params(self):
        return self.params

    def forward(self, x):
        x = np.array(x, dtype=np.float32)
        w1, b1, w2, b2, w3, b3 = self.params
        x = np.tanh(x @ w1 + b1)
        x = np.tanh(x @ w2 + b2)
        return np.tanh(x @ w3 + b3)

    def act(self, obs):
        obs = np.array(obs, dtype=np.float32)
        return self.forward(obs)

# Evaluaci√≥n individual

def evaluate_candidate(args):
    env_name, base_params, noise_i, sigma = args
    env = gym.make(env_name)
    obs_dim = env.observation_space.shape[0]
    act_dim = env.action_space.shape[0]
    policy = Policy(obs_dim, 32, act_dim)

    new_params = [p + sigma * n for p, n in zip(base_params, noise_i)]
    policy.set_params(new_params)

    total_reward = 0
    obs, _ = env.reset()
    done = False
    while not done:
        action = policy.act(obs)
        obs, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        total_reward += reward

    env.close()
    return total_reward

# Algoritmo ES con MLflow

def evolution_strategy(env_name='HalfCheetah-v4', pop_size=200, sigma=0.1, alpha=0.02, iterations=100):
    mlflow.log_param("env_name", env_name)
    mlflow.log_param("pop_size", pop_size)
    mlflow.log_param("sigma", sigma)
    mlflow.log_param("alpha", alpha)
    mlflow.log_param("iterations", iterations)

    env = gym.make(env_name)
    obs_dim = env.observation_space.shape[0]
    act_dim = env.action_space.shape[0]
    policy = Policy(obs_dim, 32, act_dim)
    reward_history = []

    for iteration in range(iterations):
        params = policy.get_params()
        noise = [[np.random.randn(*p.shape) for p in params] for _ in range(pop_size)]
        args_list = [(env_name, params, eps, sigma) for eps in noise]

        with mp.Pool(processes=mp.cpu_count()) as pool:
            rewards = pool.map(evaluate_candidate, args_list)

        rewards = np.array(rewards)
        A = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-8)

        for i in range(len(params)):
            update = sum(A[j] * noise[j][i] for j in range(pop_size))
            params[i] += alpha / (pop_size * sigma) * update

        policy.set_params(params)
        test_reward = evaluate_candidate((env_name, params, [np.zeros_like(p) for p in params], 0.0))
        reward_history.append(test_reward)
        mlflow.log_metric("test_reward", test_reward, step=iteration)
        print(f"Iteraci√≥n {iteration + 1}/{iterations}: Recompensa promedio = {test_reward:.2f}")

    env.close()
    save_results(policy, reward_history)
    plot_rewards(reward_history)
    record_video(env_name, policy)

    mlflow.log_artifact("results/paramsSwimmer-v4Para.npy")
    mlflow.log_artifact("results/rewardsSwimmer-v4Para.npy")
    mlflow.log_artifact("Swimmer-v4Para.png")
    mlflow.log_artifact("C:/Users/Usuario/Downloads/Swimmer-v4Para.mp4")

    return policy, reward_history

# Guardar video

def record_video(env_name, policy, video_path="C:\\Users\\Usuario\\Downloads\\Swimmer-v4Para.mp4"):
    env = gym.make(env_name, render_mode="rgb_array")
    obs, _ = env.reset()
    frames = []
    done = False
    while not done:
        frame = env.render()
        frames.append(frame)
        action = policy.act(obs)
        obs, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
    env.close()
    imageio.mimsave(video_path, frames, fps=30)
    print(f"üé• Video guardado en {video_path}")

# Guardar resultados

def save_results(policy, rewards, path="results"):
    os.makedirs(path, exist_ok=True)
    np.save(os.path.join(path, "paramsSwimmer-v4Para.npy"), policy.get_params())
    np.save(os.path.join(path, "rewardsSwimmer-v4Para.npy"), rewards)
    print(f"‚úÖ Resultados guardados en carpeta: {path}")

# Gr√°fica

def plot_rewards(rewards):
    plt.plot(rewards)
    plt.xlabel("Iteraci√≥n")
    plt.ylabel("Recompensa promedio")
    plt.title("Entrenamiento ES en MuJoCo")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("Swimmer-v4Para.png")
    plt.show()

# Main

if __name__ == "__main__":
    mp.set_start_method("spawn", force=True)
    mlflow.set_experiment("ES_Swimmer")

    configs = [
        {"pop_size": 100, "sigma": 0.1, "alpha": 0.02},
        {"pop_size": 200, "sigma": 0.15, "alpha": 0.02},
        {"pop_size": 150, "sigma": 0.1, "alpha": 0.01},
    ]

    for config in configs:
        with mlflow.start_run():
            policy, rewards = evolution_strategy(
                env_name="Swimmer-v4",
                pop_size=config["pop_size"],
                sigma=config["sigma"],
                alpha=config["alpha"],
                iterations=100
            )

            mlflow.sklearn.log_model(policy, "model", registered_model_name="ES_Swimmer_Model")
            client = MlflowClient()
            client.transition_model_version_stage(name="ES_Swimmer_Model", version=1, stage="Staging")
